{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"News_Model.ipynb","provenance":[],"mount_file_id":"1rATTKS2rKIZlAoTof5ezEUEpMpX_j1Rt","authorship_tag":"ABX9TyMGJJUTIjTFVnI+EndneojM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"UkjVnwUKFkk7"},"source":["!pip install transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L-w1VkjkFdXJ"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n"," \n","import torch\n","from torch import nn, optim\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n","from sklearn.model_selection import train_test_split\n"," \n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FKQdBZ89NKra"},"source":["#DEFINITION OF CONSTANTS\n","FILE_PATH = '/content/drive/MyDrive/GPUs/Bert/DataSet/News_DataSet/41_CLASSES/News41k.csv'\n","RANDOM_SEED = 42\n","MAX_LEN = 200\n","BATCH_SIZE = 16\n","NCLASSES = 41\n","PRETRAINED_BERT_MODEL = 'bert-base-cased'\n","NHIDDENS = 768\n","NAME_CLASSES = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3SylIcpNOCR"},"source":["#SETTING RANDOM VARIABLES\n","np.random.seed(RANDOM_SEED)\n","torch.manual_seed(RANDOM_SEED)\n","\n","#READING FILE\n","df = pd.read_csv(FILE_PATH)\n","\n","#Other variables\n","device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n","tokenizer = BertTokenizer.from_pretrained(PRETRAINED_BERT_MODEL)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"medwolLFNaAX"},"source":["class IMDBdataset(Dataset):\n","    \n","    def __init__(self, reviews, labels, tokenizer, max_len):\n","        self.reviews = reviews\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.reviews)\n","\n","    def __getitem__(self, item):\n","        review = str(self.reviews[item])\n","        label = self.labels[item]\n","\n","        encoding = tokenizer.encode_plus(\n","            review,\n","            max_length = self.max_len,\n","            truncation = True,\n","            add_special_tokens = True,\n","            return_token_type_ids = False,\n","            padding = 'max_length',\n","            return_attention_mask = True,\n","            return_tensors = 'pt'\n","        )\n","\n","        return {\n","            'review': review,\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'label': torch.tensor(label, dtype=torch.long)\n","        } \n","\n","def data_loader(df, tokenizer, max_len, batch_size):\n","    dataset = IMDBdataset(\n","        reviews = df.headline.to_numpy(),\n","        labels = df.label.to_numpy(),\n","        tokenizer = tokenizer,\n","        max_len = MAX_LEN\n","    )\n","\n","    return DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=2)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c-EylPiHOB8K"},"source":["df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n","df_validation, df_test = train_test_split(df_test, test_size=0.5, random_state=42)\n","\n","#Loader \n","train_data_loader = data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n","test_data_loader = data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\n","validation_data_loader = data_loader(df_validation, tokenizer, MAX_LEN, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxYPHxcXOkDB"},"source":["class BERTArticleClassificator(nn.Module):\n","\n","    def __init__(self, numClases):\n","        super(BERTArticleClassificator, self).__init__()\n","        self.bert = BertModel.from_pretrained(PRETRAINED_BERT_MODEL)\n","        self.drop = nn.Dropout(p=0.3)\n","        self.linear = nn.Linear(self.bert.config.hidden_size, NCLASSES)\n","    \n","    def forward(self, input_ids, attention_mask):\n","        outputs, cls_output = self.bert(\n","            input_ids = input_ids,\n","            attention_mask = attention_mask,\n","            return_dict = False\n","        )\n","\n","        drop_out = self.drop(cls_output)\n","        output = self.linear(drop_out)\n","\n","        return output\n","\n","model = BERTArticleClassificator(NCLASSES)\n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QjzEtiygOzLO"},"source":["EPOCHS = 25 #Iteraciones de entrenamiento\n","optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n","total_steps = len(train_data_loader) * EPOCHS #Batch_Size * EPOCHS\n","scheduler = get_linear_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps = 0,\n","    num_training_steps = total_steps\n",")\n","\n","loss_fn = nn.CrossEntropyLoss().to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WaDjABdqO39Y"},"source":["#Iteración entrenamiento\n","def train_model(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n","    model = model.train()\n","    losses = []\n","    correct_predictions = 0\n","    for batch in data_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device)\n","        output = model(input_ids=input_ids, attention_mask=attention_mask)\n","        _, preds = torch.max(output, dim=1)\n","        loss = loss_fn(output, labels)\n","        correct_predictions += torch.sum(preds == labels)\n","        losses.append(loss.item())\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        optimizer.step()\n","        scheduler.step()\n","        optimizer.zero_grad()\n","\n","    return correct_predictions.double()/n_examples, np.mean(losses)\n","\n","def eval_model(model, data_loader, loss_fn, device, n_examples):\n","    model = model.eval()\n","    losses = []\n","    correct_predictions = 0\n","    with torch.no_grad():\n","        for batch in data_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['label'].to(device)\n","            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","            _, preds = torch.max(outputs, dim=1)\n","            loss = loss_fn(outputs, labels)\n","            correct_predictions += torch.sum(preds == labels)\n","            losses.append(loss.item())\n","\n","    return correct_predictions.double()/n_examples, np.mean(losses)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"grbeYICmFZvj","outputId":"add817ba-d25c-4d10-87cc-684cd773d9cc"},"source":["for epoch in range(EPOCHS):\n","    print('Epoch {} de {}'.format(epoch+1, EPOCHS))\n","    print('-'*10)\n","    train_acc, train_loss = train_model(\n","          model, train_data_loader, loss_fn, optimizer, device, scheduler, len(df_train)\n","    )\n","    \n","    test_acc, test_loss = eval_model(\n","          model, test_data_loader, loss_fn, device, len(df_test)\n","    )\n","    \n","    print('Entrenamiento: Loss: {}, accuracy: {}'.format(train_loss, train_acc))\n","    print('Validación: Loss: {}, accuracy: {}'.format(test_loss, test_acc))\n","    print('')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1 de 25\n","----------\n"],"name":"stdout"}]}]}