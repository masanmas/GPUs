BERT TOKENIZER EXPLANATION.

Tokenizar una frase puede parecer complejo, pero existen muchos aspectos a tener en cuenta.

Por ejemplo, la frase: "Esto, es simplemente un ejemplo? O no lo es..."

Podemos realizar una primera aproximación, separando las palabras por espacios en blanco, con lo que la frase quedaría tal que:

["Esto,", "es", "simplemente", "un", "ejemplo?", "O", "no", "lo", "es..."]

El problema de esta aproximación es que nos encontramos tokens que contienen sígnos de puntuación, para evitar esto, podemos realizar una segunda aproximación, separando los signos de puntuación, la frase quedaría tal que:

["Esto", ",", "es", "simplemente", "un", "ejemplo", "?", "O", "no", "lo", "es", "..."]

Esta aproximación es bastante completa, pero tenemos que tener en cuenta el tamaño de nuestro vocabulario, ya que debemos encontrar un punto intermedio, es decir, podríamos realizar una tokenización por carácteres (["E", "s", "t", ...]) pero esto dificultaría la tarea de que nuestro modelo entendiese el contexto de los tokens, generando una gran cantidad de células en la capa de entrada de nuestra red.

Tampoco, podemos selecciónar todas las palabras de nuestro vocabulario, ya que el tamaño de este podría llegar a ser demasiado elevado, ya que el orden de palabras, en el peor de los casos es exponencial a la longitud de la sentencia, y viene dado por la siguiente expresión: TAMAÑO_DEL_VOCABULARIO = NÚMERO_CARÁCTERES ^ LONGITUD_SECUENCIA.

El algoritmo Bert, descompone las palabras en palabras de menor secuencia ya conocidas, un ejemplo muy básico sería la palabra compuesta "PARAGUAS", podríamos descomponer esta frase en los tokens "PAR" y "AGUAS", esto sucede en palabras con poca frecuencia en el texto, podríamos llegar a encontrarnos el token "PARAGUAS" sin descomponer si este aparece con mucha frecuencia.

En caso de que no se encontrase una palabra dentro del vocabulario, está se descompone en subpalabras, por ejemplo "GPU" no se encuentra dentro del vocabulario, y está se descompone en las palabras "GP" y "##U"

BERT Incluye dos modelos de tokenización:

UNCASED: Realizar un lowercase a todo el texto antes de tokenizar, además elimina los acentos de la frase. 
CASED: No realiza el lowercase y mantiene los acentos.

**El vocabulario lo diseña el propio programador.
-----------------------------------------------------------------------------------------------------------------------

WORDPIECE

Consiste en una subword Tokenization, WordPiece primero inicializa el  vocabulario con todos los carácteres, y rellena el vocabulario unas reglas que combinan dichos carácteres.

Evalua las cadenas en funcion de la probabilidad de que aparición de cada carácter en la palabra.
